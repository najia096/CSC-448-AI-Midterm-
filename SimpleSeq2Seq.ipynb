{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7pmItWRNhm0",
        "outputId": "fb9fd1f9-2a4b-4cc1-99b2-e120e84a199a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 6.920022010803223\n",
            "Epoch 2/10, Loss: 5.800600528717041\n",
            "Epoch 3/10, Loss: 4.181109428405762\n",
            "Epoch 4/10, Loss: 2.7193994522094727\n",
            "Epoch 5/10, Loss: 1.0923782587051392\n",
            "Epoch 6/10, Loss: 0.4692320227622986\n",
            "Epoch 7/10, Loss: 0.1878213733434677\n",
            "Epoch 8/10, Loss: 0.08234763145446777\n",
            "Epoch 9/10, Loss: 0.04030423238873482\n",
            "Epoch 10/10, Loss: 0.021812839433550835\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Basic Sequence-to-Sequence Model\n",
        "class SimpleSeq2Seq(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleSeq2Seq, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.rnn = nn.LSTM(hidden_size, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.rnn(embedded)\n",
        "        output = self.fc(output[:, -1, :])\n",
        "        return output\n",
        "\n",
        "# Training Function\n",
        "def train_simple_seq2seq(model, source, target, criterion, optimizer, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(source)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}')\n",
        "\n",
        "# Example Usage\n",
        "vocabulary_size = 1000\n",
        "source_sentences = torch.randint(0, vocabulary_size, (100, 10))  # 100 sentences of length 10\n",
        "target_sentences = torch.randint(0, vocabulary_size, (100,))       # Target sentences for each source sentence\n",
        "\n",
        "model = SimpleSeq2Seq(input_size=vocabulary_size, hidden_size=256, output_size=vocabulary_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "train_simple_seq2seq(model, source_sentences, target_sentences, criterion, optimizer, epochs=10)\n"
      ]
    }
  ]
}